"""
Memvid Store - The Librarian Architecture

============================================================
THE VAULT: Store Full, Raw Conversation Threads
============================================================

This implementation follows "The Librarian" pattern:

1. STORAGE: Store complete, raw conversation exchanges ("Pearls")
   - No summarization of body text
   - User inputs may be 3-page essays - preserve EVERYTHING
   - Metadata tags generated by MemoryCondenser for indexing

2. RETRIEVAL: Raw frames are passed to the Synthesizer Node
   - Search finds relevant Pearls
   - Synthesizer (gpt-4o-mini) creates detailed abstracts
   - Abstracts (200-300 words) go into prompt context

3. SOFT DELETE: Append-only with status:deleted metadata pattern
   - Corrections don't corrupt history
   - Deleted frames excluded from search results

Each AI model (persona) gets its own isolated .mv2 vault file.
"""
import os
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum

from memory_entry import (
    MemoryEntry,
    MemoryCategory,
    ImportanceLevel,
    AISelfType,
    create_memory,
    create_ai_self_memory
)

# Memvid SDK imports (v2)
try:
    from memvid_sdk import create as memvid_create, use as memvid_use
    MEMVID_AVAILABLE = True
except ImportError:
    MEMVID_AVAILABLE = False
    print("[MemvidStore] WARNING: memvid-sdk not installed. Run: pip install memvid-sdk[fastembed]")

# Import config for embedding settings
from config import config

# Default embedding model for local embeddings (fastembed)
DEFAULT_EMBEDDING_MODEL = getattr(config, 'MEMVID_EMBEDDING_MODEL', 'BAAI/bge-small-en-v1.5')

# Debug flag for tracing SDK data types
_DEBUG_SDK_TYPES = True  # Set to False after debugging

# =============================================================================
# SUPER-INDEX ARCHITECTURE CONSTANTS
# =============================================================================
# The Super-Index strategy separates:
#   - INDEX (vector_text): Compact "Fingerprint" for embedding/search
#   - VAULT (metadata): Full, untruncated conversation payload
#
# This ensures we never lose data to truncation while maintaining
# efficient semantic search via the fingerprint.

# If combined text is shorter than this, use it directly as fingerprint
SHORT_TEXT_CHAR_THRESHOLD = 1000

# Maximum characters for the fingerprint (must fit embedding model context)
# bge-small supports ~512 tokens ≈ ~2000 chars, but we use 1500 for safety margin
# This ensures we never exceed the embedding model's context window
MAX_FINGERPRINT_CHARS = 1500

# OpenAI client for fingerprint generation (initialized lazily)
_openai_client = None


# =============================================================================
# SUPER-INDEX HELPER FUNCTIONS
# =============================================================================

def _get_openai_client():
    """
    Get or create the OpenAI client for fingerprint generation.

    Uses lazy initialization to avoid overhead if fingerprints aren't needed.
    Configured via environment variables or config module.
    """
    global _openai_client
    if _openai_client is None:
        import httpx
        _openai_client = httpx.Client(
            base_url=config.OPENAI_BASE_URL,
            headers={"Authorization": f"Bearer {config.OPENAI_API_KEY}"},
            timeout=60.0
        )
    return _openai_client


def build_fingerprint(user_message: str, ai_response: str) -> str:
    """
    Build a compact "Fingerprint" for the vector index.

    The fingerprint is what gets embedded for semantic search. It must be
    compact enough to fit the embedding model's context window (~512 tokens).

    Strategy:
    - Short conversations (≤1000 chars): Use the full text directly
    - Long conversations: Generate a summary + keyword list via LLM

    Args:
        user_message: The user's message
        ai_response: The AI's response

    Returns:
        A compact fingerprint string suitable for embedding

    Invariants:
        - Output is always ≤ MAX_FINGERPRINT_CHARS
        - Output is stripped of leading/trailing whitespace
    """
    combined = f"User: {user_message}\nAI: {ai_response}"

    # Short text: use directly
    if len(combined) <= SHORT_TEXT_CHAR_THRESHOLD:
        fingerprint = combined[:MAX_FINGERPRINT_CHARS]
        if _DEBUG_SDK_TYPES:
            print(f"[Fingerprint] Short text ({len(combined)} chars), using directly")
        return fingerprint.strip()

    # Long text: generate summary + keywords via LLM
    if _DEBUG_SDK_TYPES:
        print(f"[Fingerprint] Long text ({len(combined)} chars), generating summary...")

    try:
        client = _get_openai_client()

        # Truncate input to avoid token limits on the LLM call
        truncated_input = combined[:8000]  # ~2000 tokens input limit

        prompt = f"""You are creating a search index entry for a conversation between a user and an AI assistant.

Write a concise semantic summary of the core ideas in about 150 words.
Then append a line starting with 'Keywords:' followed by a comma-separated list of ALL specific entities, proper nouns, and distinct concepts mentioned (even minor ones, e.g., 'Blueberries', 'Spock', 'Romans 8').

Output format:
Summary: <one paragraph summary>
Keywords: <comma-separated keyword list>

CONVERSATION:
{truncated_input}"""

        response = client.post(
            "/chat/completions",
            json={
                "model": getattr(config, 'OPENAI_CONDENSER_MODEL', 'gpt-4o-mini'),
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 500
            }
        )
        response.raise_for_status()

        result = response.json()
        fingerprint = result["choices"][0]["message"]["content"].strip()

        if _DEBUG_SDK_TYPES:
            print(f"[Fingerprint] Generated {len(fingerprint)} char summary")

    except Exception as e:
        print(f"[Fingerprint] LLM call failed: {e}, using truncated text")
        # Fallback: use truncated combined text
        fingerprint = combined

    # Hard truncate to ensure we never exceed embedding model limits
    fingerprint = fingerprint[:MAX_FINGERPRINT_CHARS].strip()

    return fingerprint


def payload_to_text(full_payload: dict) -> str:
    """
    Reconstruct the full conversation text from a stored payload.

    This is the inverse of storing: it takes the full_payload from metadata
    and returns the complete, untruncated conversation string.

    Args:
        full_payload: Dict with 'user' and 'ai' keys containing full messages

    Returns:
        Full conversation text: "User: {message}\nAI: {response}"
    """
    if not full_payload:
        return ""

    user = full_payload.get("user", "")
    ai = full_payload.get("ai", "")

    return f"User: {user}\nAI: {ai}"


def _safe_parse_metadata(meta: Any, debug_context: str = "") -> dict:
    """
    Safely parse metadata from Memvid SDK.

    The SDK may return metadata as a JSON string or as a dict.
    This helper ensures we always get a dict.

    Args:
        meta: The metadata value (could be str, dict, or None)
        debug_context: Context string for debug logging

    Returns:
        Parsed metadata dict
    """
    if meta is None:
        return {}

    if isinstance(meta, dict):
        return meta

    if isinstance(meta, str):
        # Metadata came back as JSON string - parse it
        if _DEBUG_SDK_TYPES:
            print(f"[Librarian DEBUG] Metadata is string in {debug_context}, parsing JSON...")
        try:
            parsed = json.loads(meta)
            if isinstance(parsed, dict):
                return parsed
            else:
                print(f"[Librarian] Warning: Parsed metadata is {type(parsed)}, expected dict")
                return {}
        except json.JSONDecodeError as e:
            print(f"[Librarian] Warning: Failed to parse metadata JSON: {e}")
            return {}

    # Unknown type
    if _DEBUG_SDK_TYPES:
        print(f"[Librarian DEBUG] Unexpected metadata type in {debug_context}: {type(meta)}")
    return {}


def _safe_parse_hit(hit: Any, debug_context: str = "") -> dict:
    """
    Safely parse a search hit from Memvid SDK.

    Args:
        hit: A search hit (could be dict or other)
        debug_context: Context for debug logging

    Returns:
        Hit as a dict
    """
    if hit is None:
        return {}

    if isinstance(hit, dict):
        return hit

    if isinstance(hit, str):
        if _DEBUG_SDK_TYPES:
            print(f"[Librarian DEBUG] Hit is string in {debug_context}, parsing JSON...")
        try:
            return json.loads(hit)
        except json.JSONDecodeError:
            return {"text": hit}

    # If it has a to_dict method, use it
    if hasattr(hit, 'to_dict'):
        return hit.to_dict()

    # If it has __dict__, use that
    if hasattr(hit, '__dict__'):
        return vars(hit)

    if _DEBUG_SDK_TYPES:
        print(f"[Librarian DEBUG] Unexpected hit type in {debug_context}: {type(hit)}")
        print(f"[Librarian DEBUG] Hit value: {str(hit)[:200]}")

    return {}


def _extract_hits_from_result(result: Any, debug_context: str = "") -> List[dict]:
    """
    Extract the hits array from a Memvid SDK find() result.

    The SDK returns: {'query': '...', 'hits': [...], 'took_ms': ..., ...}
    This helper extracts just the hits list.

    Args:
        result: The result from mv.find()
        debug_context: Context for debug logging

    Returns:
        List of hit dicts
    """
    if result is None:
        return []

    # If result is already a list, return it
    if isinstance(result, list):
        return result

    # If result is a dict with 'hits' key, extract it
    if isinstance(result, dict):
        hits = result.get("hits", [])
        if _DEBUG_SDK_TYPES and hits:
            print(f"[Librarian DEBUG] Extracted {len(hits)} hits from find() result in {debug_context}")
        return hits if isinstance(hits, list) else []

    # If it's a string, try parsing as JSON
    if isinstance(result, str):
        try:
            parsed = json.loads(result)
            return _extract_hits_from_result(parsed, debug_context)
        except json.JSONDecodeError:
            pass

    if _DEBUG_SDK_TYPES:
        print(f"[Librarian DEBUG] Unexpected find() result type in {debug_context}: {type(result)}")

    return []


class FrameStatus(str, Enum):
    """Status of a memory frame (Pearl)."""
    ACTIVE = "active"      # Normal, retrievable frame
    DELETED = "deleted"    # Soft-deleted, excluded from search
    ARCHIVED = "archived"  # Superseded by newer version
    DRAFT = "draft"        # Not yet finalized


@dataclass
class Pearl:
    """
    A Pearl is a complete conversation exchange (frame) in the vault.

    Pearls store the FULL, RAW conversation - no summarization.
    Metadata tags are used for indexing and retrieval.
    """
    id: str
    user_message: str           # Full user input (may be 3+ pages)
    ai_response: str            # Full AI response
    tags: List[str] = field(default_factory=list)  # #Theology, #Core, etc.
    category: str = "context"
    importance: str = "normal"
    emotional_tone: Optional[str] = None
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    status: str = FrameStatus.ACTIVE.value
    user_name: str = "User"
    model_id: Optional[str] = None

    # Additional metadata
    thread_id: Optional[str] = None      # Conversation thread this belongs to
    message_index: Optional[int] = None  # Position in thread
    word_count: int = 0

    def __post_init__(self):
        """Calculate word count on creation."""
        self.word_count = len(self.user_message.split()) + len(self.ai_response.split())

    @property
    def full_content(self) -> str:
        """
        Get the complete raw content for display/synthesis.

        Note: With Super-Index architecture, this is NOT used for storage.
        Storage uses full_payload in metadata (never truncated).
        This property is for display and passing to the Synthesizer.
        """
        return f"User: {self.user_message}\nAI: {self.ai_response}"

    def get_label(self) -> str:
        """
        Generate the Memvid label for fast filtering.

        Format: "category:{cat}|importance:{imp}|status:{status}"
        """
        tag_str = "|".join(f"tag:{t}" for t in self.tags[:5])  # Limit tags in label
        return f"category:{self.category}|importance:{self.importance}|status:{self.status}|{tag_str}"

    def get_metadata(self) -> dict:
        """
        Get full metadata dict for Memvid storage.

        SUPER-INDEX ARCHITECTURE:
        - full_payload: Contains complete, untruncated user + AI messages
        - Other fields: Indexing metadata for filtering and organization

        The full_payload is the source of truth for retrieval.
        The vector_text (fingerprint) is only used for search, not storage.
        """
        return {
            # === VAULT: Full payload for retrieval (NEVER truncated) ===
            "full_payload": {
                "user": self.user_message,
                "ai": self.ai_response,
            },
            # === INDEXING METADATA ===
            "tags": self.tags,
            "category": self.category,
            "importance": self.importance,
            "emotional_tone": self.emotional_tone,
            "created_at": self.created_at,
            "status": self.status,
            "user_name": self.user_name,
            "thread_id": self.thread_id,
            "message_index": self.message_index,
            "word_count": self.word_count,
            "model_id": self.model_id
        }

    def to_dict(self) -> dict:
        """Convert to dictionary for serialization."""
        return {
            "pearl_id": self.id,
            "user_message": self.user_message,
            "ai_response": self.ai_response,
            "tags": self.tags,
            "category": self.category,
            "importance": self.importance,
            "emotional_tone": self.emotional_tone,
            "created_at": self.created_at,
            "status": self.status,
            "user_name": self.user_name,
            "thread_id": self.thread_id,
            "message_index": self.message_index,
            "word_count": self.word_count,
            "full_content": self.full_content
        }

    @classmethod
    def from_memvid_hit(cls, hit: Any, model_id: str) -> "Pearl":
        """
        Create a Pearl from a Memvid search hit.

        SUPER-INDEX ARCHITECTURE:
        - The hit.text contains the fingerprint (for search), NOT full content
        - The hit.metadata contains full_payload with complete conversation
        - We ALWAYS read from full_payload, never parse hit.text for content

        This ensures we never lose data to truncation.
        """
        # Safely parse the hit (may be dict or other type from SDK)
        hit_dict = _safe_parse_hit(hit, "from_memvid_hit")

        # Get the text field (for debug only - this is the fingerprint, not full content)
        text = hit_dict.get("text", hit_dict.get("snippet", ""))

        if _DEBUG_SDK_TYPES:
            print(f"\n[Librarian DEBUG] ========== SUPER-INDEX RETRIEVAL ==========")
            print(f"[Librarian DEBUG] Fingerprint (text field): {len(text)} chars")
            print(f"[Librarian DEBUG] Hit keys: {hit_dict.keys()}")

        # === PARSE METADATA ===
        # The SDK serializes metadata into the text field, so we need to extract it
        raw_meta = hit_dict.get("metadata", {})
        meta = _safe_parse_metadata(raw_meta, "from_memvid_hit.metadata")

        if _DEBUG_SDK_TYPES:
            print(f"[Librarian DEBUG] Metadata keys: {meta.keys() if meta else 'None'}")

        # === SUPER-INDEX: READ FROM full_payload (PRIMARY METHOD) ===
        # This is the source of truth - complete, untruncated content
        user_message = ""
        ai_response = ""

        full_payload = meta.get("full_payload")
        if full_payload:
            # full_payload may be a dict or JSON string
            if isinstance(full_payload, str):
                try:
                    full_payload = json.loads(full_payload)
                except json.JSONDecodeError:
                    full_payload = None

            if isinstance(full_payload, dict):
                user_message = full_payload.get("user", "")
                ai_response = full_payload.get("ai", "")
                if _DEBUG_SDK_TYPES:
                    print(f"[Librarian DEBUG] SUCCESS: Read from full_payload")
                    print(f"[Librarian DEBUG]   user: {len(user_message)} chars")
                    print(f"[Librarian DEBUG]   ai: {len(ai_response)} chars")

        # === FALLBACK: Try legacy metadata fields ===
        if not user_message and not ai_response:
            user_message = meta.get("user_message", "") or meta.get("user", "")
            ai_response = meta.get("ai_response", "") or meta.get("ai", "")
            if user_message or ai_response:
                if _DEBUG_SDK_TYPES:
                    print(f"[Librarian DEBUG] FALLBACK: Read from legacy metadata fields")

        # === FALLBACK: Try parsing from serialized text (for very old data) ===
        if not user_message and not ai_response:
            import re
            if _DEBUG_SDK_TYPES:
                print(f"[Librarian DEBUG] FALLBACK: Trying to parse from serialized text")

            # Try regex extraction from SDK's serialized format
            user_match = re.search(r'"user":\s*"((?:[^"\\]|\\.)*)"', text)
            if user_match:
                user_message = user_match.group(1).replace('\\"', '"').replace('\\n', '\n')

            ai_match = re.search(r'"ai":\s*"((?:[^"\\]|\\.)*)"', text)
            if ai_match:
                ai_response = ai_match.group(1).replace('\\"', '"').replace('\\n', '\n')

        # === FINAL FALLBACK: Use fingerprint text as user message ===
        if not user_message and not ai_response:
            if _DEBUG_SDK_TYPES:
                print(f"[Librarian DEBUG] LAST RESORT: Using fingerprint as content")
            # Strip metadata suffix if present
            if " title:" in text:
                user_message = text.split(" title:")[0].strip()
            else:
                user_message = text

        if _DEBUG_SDK_TYPES:
            print(f"[Librarian DEBUG] FINAL: user={len(user_message)} chars, ai={len(ai_response)} chars")
            print(f"[Librarian DEBUG] ==========================================\n")

        # === PARSE OTHER METADATA ===
        tags = meta.get("tags", [])
        if isinstance(tags, str):
            try:
                tags = json.loads(tags)
            except:
                tags = [tags] if tags else []

        # Parse from label as fallback
        label = hit_dict.get("label", "")
        category = meta.get("category", "context")
        importance = meta.get("importance", "normal")
        status = meta.get("status", FrameStatus.ACTIVE.value)

        if label:
            for part in label.split("|"):
                if part.startswith("category:"):
                    category = part.replace("category:", "")
                elif part.startswith("importance:"):
                    importance = part.replace("importance:", "")
                elif part.startswith("status:"):
                    status = part.replace("status:", "")
                elif part.startswith("tag:"):
                    tag = part.replace("tag:", "")
                    if tag and tag not in tags:
                        tags.append(tag)

        return cls(
            id=hit_dict.get("frame_id", hit_dict.get("title", "")),
            user_message=user_message,
            ai_response=ai_response,
            tags=tags,
            category=category,
            importance=importance,
            emotional_tone=meta.get("emotional_tone"),
            created_at=meta.get("created_at", datetime.now().isoformat()),
            status=status,
            user_name=meta.get("user_name", "User"),
            thread_id=meta.get("thread_id"),
            message_index=meta.get("message_index"),
            word_count=meta.get("word_count", 0),
            model_id=model_id
        )

    def is_deleted(self) -> bool:
        """Check if this Pearl is soft-deleted."""
        return self.status == FrameStatus.DELETED.value

    def is_active(self) -> bool:
        """Check if this Pearl is active (retrievable)."""
        return self.status == FrameStatus.ACTIVE.value


@dataclass
class SearchResult:
    """A single search result from the vault."""
    pearl: Pearl
    score: float
    preview: str

    # Legacy compatibility
    @property
    def memory(self) -> MemoryEntry:
        """Convert Pearl to MemoryEntry for backward compatibility."""
        return MemoryEntry(
            id=self.pearl.id,
            content=self.pearl.full_content,
            category=self.pearl.category,
            importance=self.pearl.importance,
            tags=self.pearl.tags,
            created_at=self.pearl.created_at,
            source="vault"
        )

    def to_dict(self) -> dict:
        return {
            "pearl": self.pearl.to_dict(),
            "score": self.score,
            "preview": self.preview
        }


class MemvidStore:
    """
    The Librarian: Memory vault for a single AI model/persona.

    Stores FULL, RAW conversation exchanges (Pearls) without summarization.
    Uses metadata tags for indexing. Retrieval goes through the Synthesizer.

    Architecture:
    - Storage: Raw Pearls (complete exchanges)
    - Indexing: Metadata tags (#Theology, #Core, etc.)
    - Retrieval: Search → Raw Pearls → Synthesizer → Abstract
    - Deletion: Soft delete via status:deleted metadata
    """

    def __init__(
        self,
        model_id: str,
        vaults_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None
    ):
        """
        Initialize or open the vault for a model.

        Args:
            model_id: The AI model/persona identifier (e.g., "eli", "opus")
            vaults_dir: Directory for vault files (default: ./data/vaults)
            embedding_model: Embedding model for vector search (default: bge-small-en-v1.5)
        """
        if not MEMVID_AVAILABLE:
            raise ImportError(
                "memvid-sdk is not installed. Install with:\n"
                "pip install memvid-sdk[fastembed]"
            )

        self.model_id = model_id
        self._safe_model_id = model_id.replace("/", "_").replace("\\", "_")

        # Embedding model for vector search (local fastembed)
        self.embedding_model = embedding_model or DEFAULT_EMBEDDING_MODEL

        # Set up vault directory
        if vaults_dir is None:
            vaults_dir = Path("./data/vaults")
        self.vaults_dir = Path(vaults_dir)
        self.vaults_dir.mkdir(parents=True, exist_ok=True)

        # Vault file path
        self.vault_path = self.vaults_dir / f"{self._safe_model_id}.mv2"

        # Initialize the Memvid connection
        self._mv = None
        self._last_update: Optional[str] = None
        self._pearl_count: int = 0

        self._open_vault()

    def _open_vault(self):
        """Open or create the vault file with vector indexing enabled."""
        path_str = str(self.vault_path)

        # Embedding configuration for local HuggingFace embeddings
        # This enables hybrid search (lexical + semantic) without external API calls
        apikey_config = {
            "embedding": {
                "provider": "huggingface",
                "model": self.embedding_model
            }
        }

        try:
            if self.vault_path.exists():
                print(f"[Librarian] Opening vault: {self.vault_path}")
                print(f"[Librarian] Using embedding model: {self.embedding_model}")
                # Open existing vault with vector search enabled
                self._mv = memvid_use(
                    'basic',
                    path_str,
                    apikey=apikey_config,
                    enable_vec=True,
                    enable_lex=True
                )
            else:
                print(f"[Librarian] Creating new vault: {self.vault_path}")
                print(f"[Librarian] Embedding model: {self.embedding_model}")
                # Create new vault with vector + lexical indexing enabled
                self._mv = memvid_create(
                    path_str,
                    apikey=apikey_config,
                    enable_vec=True,
                    enable_lex=True
                )

            self._refresh_stats()
        except Exception as e:
            print(f"[Librarian] Error opening vault: {e}")
            raise

    def _refresh_stats(self):
        """Refresh internal statistics from the vault."""
        try:
            if self._mv and hasattr(self._mv, 'stats'):
                stats = self._mv.stats()
                self._pearl_count = stats.get('document_count', 0)
            self._last_update = datetime.now().isoformat()
        except Exception as e:
            print(f"[Librarian] Warning: Could not refresh stats: {e}")

    def close(self):
        """Close the vault connection."""
        if self._mv and hasattr(self._mv, 'seal'):
            try:
                self._mv.seal()
            except Exception as e:
                print(f"[Librarian] Warning: Error closing vault: {e}")
        self._mv = None

    # =========================================================================
    # STORAGE: Add Raw Pearls (Full Conversation Exchanges)
    # =========================================================================

    def add_pearl(
        self,
        user_message: str,
        ai_response: str,
        tags: Optional[List[str]] = None,
        category: str = "context",
        importance: str = "normal",
        emotional_tone: Optional[str] = None,
        created_at: Optional[str] = None,
        user_name: str = "User",
        thread_id: Optional[str] = None,
        message_index: Optional[int] = None,
        pearl_id: Optional[str] = None
    ) -> str:
        """
        Add a complete conversation exchange (Pearl) to the vault.

        SUPER-INDEX ARCHITECTURE:
        - INDEX (text field): Compact "Fingerprint" for embedding/search
          - Short conversations: full text
          - Long conversations: LLM-generated summary + keywords
        - VAULT (metadata.full_payload): Complete, untruncated conversation

        This ensures:
        1. Semantic search works efficiently (fingerprint fits embedding context)
        2. Full fidelity retrieval (complete text in metadata, never truncated)

        Args:
            user_message: The complete user input (may be 3+ pages)
            ai_response: The complete AI response
            tags: Metadata tags for indexing (#Theology, #Core, etc.)
            category: Category for organization
            importance: Importance level (core, high, normal, low)
            emotional_tone: Emotional context of the exchange
            created_at: Original timestamp (ISO format) - PRESERVED!
            user_name: User's name for the transcript
            thread_id: Optional thread/conversation ID
            message_index: Position in the conversation thread
            pearl_id: Optional explicit ID (auto-generated if not provided)

        Returns:
            The Pearl ID
        """
        # Generate ID if not provided
        if not pearl_id:
            pearl_id = f"pearl_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"

        pearl = Pearl(
            id=pearl_id,
            user_message=user_message,
            ai_response=ai_response,
            tags=tags or [],
            category=category,
            importance=importance,
            emotional_tone=emotional_tone,
            created_at=created_at or datetime.now().isoformat(),
            status=FrameStatus.ACTIVE.value,
            user_name=user_name,
            thread_id=thread_id,
            message_index=message_index,
            model_id=self.model_id
        )

        # === SUPER-INDEX: Build fingerprint for vector search ===
        # The fingerprint is compact enough for embedding models
        # Full content is stored in metadata.full_payload
        fingerprint = build_fingerprint(user_message, ai_response)

        if _DEBUG_SDK_TYPES:
            print(f"[Librarian] Fingerprint: {len(fingerprint)} chars (original: {pearl.word_count} words)")

        # Store in Memvid
        # - text: fingerprint (for embedding/search)
        # - metadata: full_payload + indexing metadata (for retrieval)
        self._mv.put(
            title=pearl.id,
            label=pearl.get_label(),
            metadata=pearl.get_metadata(),  # Contains full_payload with complete text
            text=fingerprint  # Compact fingerprint for embedding
        )

        self._pearl_count += 1
        self._last_update = datetime.now().isoformat()

        tag_str = ", ".join(tags[:3]) if tags else "none"
        print(f"[Librarian] Stored Pearl [{pearl.id}] {pearl.word_count} words, tags: {tag_str}")
        return pearl.id

    def add_pearl_object(self, pearl: Pearl) -> str:
        """
        Add a pre-constructed Pearl to the vault.

        Uses Super-Index architecture: fingerprint for search, full_payload for retrieval.
        """
        pearl.model_id = self.model_id

        # Build fingerprint for this pearl
        fingerprint = build_fingerprint(pearl.user_message, pearl.ai_response)

        self._mv.put(
            title=pearl.id,
            label=pearl.get_label(),
            metadata=pearl.get_metadata(),
            text=fingerprint
        )

        self._pearl_count += 1
        self._last_update = datetime.now().isoformat()

        return pearl.id

    def add_many_pearls(self, pearls: List[Pearl]) -> List[str]:
        """Batch add multiple Pearls efficiently using Super-Index architecture."""
        if not pearls:
            return []

        batch = []
        for pearl in pearls:
            pearl.model_id = self.model_id
            fingerprint = build_fingerprint(pearl.user_message, pearl.ai_response)
            batch.append({
                "title": pearl.id,
                "label": pearl.get_label(),
                "metadata": pearl.get_metadata(),
                "text": fingerprint  # Use fingerprint, not full_content
            })

        if hasattr(self._mv, 'put_many'):
            self._mv.put_many(batch)
        else:
            for item in batch:
                self._mv.put(**item)

        self._pearl_count += len(pearls)
        self._last_update = datetime.now().isoformat()

        print(f"[Librarian] Batch stored {len(pearls)} Pearls")
        return [p.id for p in pearls]

    # =========================================================================
    # LEGACY COMPATIBILITY: add_memory wraps add_pearl
    # =========================================================================

    def add_memory(
        self,
        content: str,
        category: str = "context",
        importance: str = "normal",
        tags: Optional[List[str]] = None,
        user_id: Optional[str] = None,
        source: str = "conversation",
        created_at: Optional[str] = None,
        ai_self_type: Optional[str] = None,
        supersedes: Optional[str] = None,
        emotional_tone: Optional[str] = None,
        memory_id: Optional[str] = None
    ) -> str:
        """
        Legacy compatibility: Add a memory (converts to Pearl format).

        For new code, use add_pearl() directly.
        """
        # Parse content into user_message and ai_response if possible
        user_message = content
        ai_response = ""

        if "\nAI:" in content or "\nAI responded:" in content:
            # Try to split into user/AI parts
            for splitter in ["\nAI:", "\nAI responded:", "\n\nAI:"]:
                if splitter in content:
                    parts = content.split(splitter, 1)
                    user_message = parts[0].replace("User:", "").replace("User said:", "").strip()
                    ai_response = parts[1].strip()
                    break

        return self.add_pearl(
            user_message=user_message,
            ai_response=ai_response,
            tags=tags or [],
            category=category,
            importance=importance,
            emotional_tone=emotional_tone,
            created_at=created_at,
            pearl_id=memory_id
        )

    def add_memory_entry(self, entry: MemoryEntry) -> str:
        """Legacy compatibility: Add a MemoryEntry (converts to Pearl)."""
        return self.add_memory(
            content=entry.content,
            category=entry.category,
            importance=entry.importance,
            tags=entry.tags,
            created_at=entry.created_at,
            emotional_tone=entry.emotional_tone,
            memory_id=entry.id
        )

    def add_many(self, entries: List[MemoryEntry]) -> List[str]:
        """Legacy compatibility: Batch add MemoryEntries."""
        return [self.add_memory_entry(e) for e in entries]

    # =========================================================================
    # SOFT DELETE: Mark Pearls as deleted without removing
    # =========================================================================

    def soft_delete(self, pearl_id: str, reason: Optional[str] = None) -> bool:
        """
        Soft delete a Pearl by adding a deletion marker.

        In append-only storage, we add a new record marking the old one deleted.
        The original content is preserved but excluded from search results.

        Args:
            pearl_id: ID of the Pearl to delete
            reason: Optional reason for deletion

        Returns:
            True if deletion marker was added
        """
        try:
            # Add a deletion marker frame
            deletion_id = f"delete_{pearl_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            self._mv.put(
                title=deletion_id,
                label=f"deletion_marker|target:{pearl_id}|status:deleted",
                metadata={
                    "type": "deletion_marker",
                    "target_pearl_id": pearl_id,
                    "deleted_at": datetime.now().isoformat(),
                    "reason": reason or "user_requested"
                },
                text=f"DELETION MARKER: Pearl {pearl_id} has been soft-deleted. Reason: {reason or 'user requested'}"
            )

            print(f"[Librarian] Soft deleted Pearl: {pearl_id}")
            return True

        except Exception as e:
            print(f"[Librarian] Error soft-deleting Pearl {pearl_id}: {e}")
            return False

    def get_deleted_pearl_ids(self) -> set:
        """Get set of all soft-deleted Pearl IDs."""
        deleted_ids = set()

        try:
            # Search for deletion markers
            result = self._mv.find("deletion_marker", k=1000, mode="lex")
            hits = _extract_hits_from_result(result, "get_deleted_pearl_ids")

            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_deleted_pearl_ids.hit")
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    # Extract target pearl ID
                    for part in label.split("|"):
                        if part.startswith("target:"):
                            deleted_ids.add(part.replace("target:", ""))

        except Exception as e:
            print(f"[Librarian] Error getting deleted IDs: {e}")

        return deleted_ids

    # =========================================================================
    # SEARCH: Find relevant Pearls (for Synthesizer)
    # =========================================================================

    def search_pearls(
        self,
        query: str,
        limit: int = 10,
        mode: str = "hybrid",
        include_deleted: bool = False
    ) -> List[SearchResult]:
        """
        Search for relevant Pearls (complete conversation exchanges).

        These raw Pearls should be passed to the Synthesizer for abstraction
        before being injected into the prompt context.

        Args:
            query: The search query
            limit: Maximum results to return
            mode: Search mode - "hybrid" (default), "lex", "sem"
            include_deleted: Whether to include soft-deleted Pearls

        Returns:
            List of SearchResult objects with full Pearls
        """
        if not query.strip():
            return []

        try:
            # Get deleted IDs to filter
            deleted_ids = set() if include_deleted else self.get_deleted_pearl_ids()

            memvid_mode = {
                "hybrid": None,
                "lex": "lex",
                "lexical": "lex",
                "sem": "sem",
                "semantic": "sem",
                "vector": "sem"
            }.get(mode.lower())

            if memvid_mode:
                find_result = self._mv.find(query, k=limit * 2, mode=memvid_mode)
            else:
                find_result = self._mv.find(query, k=limit * 2)

            hits = _extract_hits_from_result(find_result, "search_pearls")

            results = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "search_pearls.hit")

                # Skip deletion markers
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    continue

                pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)

                # Skip deleted pearls
                if pearl.id in deleted_ids:
                    continue

                # Skip non-active status
                if not include_deleted and not pearl.is_active():
                    continue

                results.append(SearchResult(
                    pearl=pearl,
                    score=hit.get("score", 0.0),
                    preview=hit.get("preview", pearl.full_content[:200])
                ))

                if len(results) >= limit:
                    break

            return results

        except Exception as e:
            print(f"[Librarian] Search error: {e}")
            return []

    def search(
        self,
        query: str,
        limit: int = 10,
        mode: str = "hybrid",
        exclude_archived: bool = True
    ) -> List[SearchResult]:
        """Legacy compatibility: Alias for search_pearls."""
        return self.search_pearls(query, limit, mode, include_deleted=not exclude_archived)

    def get_pearls_by_tag(self, tag: str, limit: int = 50) -> List[Pearl]:
        """Get Pearls with a specific tag."""
        try:
            # Search for tag in label
            find_result = self._mv.find(f"tag:{tag}", k=limit * 2, mode="lex")
            hits = _extract_hits_from_result(find_result, "get_pearls_by_tag")
            deleted_ids = self.get_deleted_pearl_ids()

            results = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_pearls_by_tag.hit")
                if "deletion_marker" in hit_dict.get("label", ""):
                    continue

                pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)

                if pearl.id in deleted_ids:
                    continue

                if tag in pearl.tags or f"#{tag}" in pearl.tags:
                    results.append(pearl)

                if len(results) >= limit:
                    break

            return results

        except Exception as e:
            print(f"[Librarian] Error getting tag {tag}: {e}")
            return []

    def get_core_pearls(self) -> List[Pearl]:
        """Get all core (always-included) Pearls."""
        try:
            find_result = self._mv.find("importance:core", k=100, mode="lex")
            hits = _extract_hits_from_result(find_result, "get_core_pearls")
            deleted_ids = self.get_deleted_pearl_ids()

            results = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_core_pearls.hit")
                label = hit_dict.get("label", "")
                if "importance:core" in label and "deletion_marker" not in label:
                    title = hit_dict.get("title", "")
                    if title not in deleted_ids:
                        results.append(Pearl.from_memvid_hit(hit_dict, self.model_id))
            return results
        except Exception as e:
            print(f"[Librarian] Error getting core Pearls: {e}")
            return []

    def get_by_category(self, category: str, limit: int = 50) -> List[MemoryEntry]:
        """Legacy compatibility: Get memories by category."""
        try:
            find_result = self._mv.find(f"category:{category}", k=limit, mode="lex")
            hits = _extract_hits_from_result(find_result, "get_by_category")
            deleted_ids = self.get_deleted_pearl_ids()

            entries = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_by_category.hit")
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    continue
                if f"category:{category}" not in label:
                    continue
                if hit_dict.get("title", "") in deleted_ids:
                    continue

                pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)
                entries.append(MemoryEntry(
                    id=pearl.id,
                    content=pearl.full_content,
                    category=pearl.category,
                    importance=pearl.importance,
                    tags=pearl.tags,
                    created_at=pearl.created_at
                ))

            return entries

        except Exception as e:
            print(f"[Librarian] Error getting category {category}: {e}")
            return []

    def get_by_importance(self, importance: str, limit: int = 50) -> List[MemoryEntry]:
        """Legacy compatibility: Get memories by importance."""
        try:
            find_result = self._mv.find(f"importance:{importance}", k=limit, mode="lex")
            hits = _extract_hits_from_result(find_result, "get_by_importance")
            deleted_ids = self.get_deleted_pearl_ids()

            entries = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_by_importance.hit")
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    continue
                if f"importance:{importance}" not in label:
                    continue
                if hit_dict.get("title", "") in deleted_ids:
                    continue

                pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)
                entries.append(MemoryEntry(
                    id=pearl.id,
                    content=pearl.full_content,
                    category=pearl.category,
                    importance=pearl.importance,
                    tags=pearl.tags,
                    created_at=pearl.created_at
                ))

            return entries

        except Exception as e:
            print(f"[Librarian] Error getting importance {importance}: {e}")
            return []

    def get_core_memories(self) -> List[MemoryEntry]:
        """Legacy compatibility: Get core memories."""
        pearls = self.get_core_pearls()
        return [
            MemoryEntry(
                id=p.id,
                content=p.full_content,
                category=p.category,
                importance=p.importance,
                tags=p.tags,
                created_at=p.created_at
            )
            for p in pearls
        ]

    def get_ai_self_memories(self, include_archived: bool = False) -> List[MemoryEntry]:
        """Legacy compatibility: Get AI self memories."""
        return self.get_by_category(MemoryCategory.AI_SELF.value, limit=200)

    def get_ai_self_by_type(self, ai_self_type: str) -> List[MemoryEntry]:
        """Legacy compatibility: Get AI self memories by type."""
        all_ai_self = self.get_ai_self_memories()
        return [m for m in all_ai_self if ai_self_type in m.tags]

    def get_recent(self, limit: int = 10) -> List[MemoryEntry]:
        """Get most recent memories."""
        try:
            deleted_ids = self.get_deleted_pearl_ids()
            hits = []

            # Try timeline first, with specific handling for MV005 (vault too small)
            if hasattr(self._mv, 'timeline'):
                try:
                    result = self._mv.timeline(limit=limit * 2)
                    hits = _extract_hits_from_result(result, "get_recent.timeline")
                except Exception as timeline_err:
                    err_str = str(timeline_err)
                    if "MV005" in err_str:
                        # MV005: Time index track is invalid - vault too small for timeline
                        if _DEBUG_SDK_TYPES:
                            print(f"[Librarian DEBUG] MV005 ignored: Vault too small for timeline")
                        # Fall through to lexical search
                    else:
                        # Re-raise unexpected errors
                        raise

            # Fallback to lexical search if timeline failed or returned nothing
            if not hits:
                result = self._mv.find("", k=limit * 3, mode="lex")
                hits = _extract_hits_from_result(result, "get_recent.find")

            entries = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_recent.hit")
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    continue
                if hit_dict.get("title", "") in deleted_ids:
                    continue

                pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)
                entries.append(MemoryEntry(
                    id=pearl.id,
                    content=pearl.full_content,
                    category=pearl.category,
                    importance=pearl.importance,
                    tags=pearl.tags,
                    created_at=pearl.created_at
                ))

            entries.sort(key=lambda e: e.created_at or "", reverse=True)
            return entries[:limit]

        except Exception as e:
            # Check for MV005 at top level too (defensive)
            if "MV005" in str(e):
                if _DEBUG_SDK_TYPES:
                    print(f"[Librarian DEBUG] MV005 ignored: Vault too small for timeline")
                return []
            print(f"[Librarian] Error getting recent: {e}")
            return []

    def get_by_tag(self, tag: str, limit: int = 50) -> List[MemoryEntry]:
        """Legacy compatibility: Get memories by tag."""
        pearls = self.get_pearls_by_tag(tag, limit)
        return [
            MemoryEntry(
                id=p.id,
                content=p.full_content,
                category=p.category,
                importance=p.importance,
                tags=p.tags,
                created_at=p.created_at
            )
            for p in pearls
        ]

    # =========================================================================
    # RAW PEARL RETRIEVAL (for Synthesizer)
    # =========================================================================

    def get_raw_pearls_for_synthesis(
        self,
        query: str,
        limit: int = 5
    ) -> List[Pearl]:
        """
        Get raw Pearls for the Synthesizer to process.

        This is the key method for the Librarian architecture:
        1. Search finds relevant Pearls
        2. Return full, raw content
        3. Caller passes to Synthesizer for abstraction

        Args:
            query: Search query
            limit: Maximum Pearls to return

        Returns:
            List of full Pearl objects with complete content
        """
        results = self.search_pearls(query, limit=limit, mode="hybrid")
        return [r.pearl for r in results]

    def get_thread_pearls(self, thread_id: str) -> List[Pearl]:
        """Get all Pearls from a specific conversation thread."""
        try:
            find_result = self._mv.find(f"thread:{thread_id}", k=100, mode="lex")
            hits = _extract_hits_from_result(find_result, "get_thread_pearls")
            deleted_ids = self.get_deleted_pearl_ids()

            pearls = []
            for hit in hits:
                hit_dict = _safe_parse_hit(hit, "get_thread_pearls.hit")
                label = hit_dict.get("label", "")
                if "deletion_marker" in label:
                    continue

                meta = _safe_parse_metadata(hit_dict.get("metadata", {}), "get_thread_pearls.metadata")
                if meta.get("thread_id") != thread_id:
                    continue

                if hit_dict.get("title", "") in deleted_ids:
                    continue

                pearls.append(Pearl.from_memvid_hit(hit_dict, self.model_id))

            # Sort by message index
            pearls.sort(key=lambda p: p.message_index or 0)
            return pearls

        except Exception as e:
            print(f"[Librarian] Error getting thread {thread_id}: {e}")
            return []

    # =========================================================================
    # STATISTICS AND EXPORT
    # =========================================================================

    def get_stats(self) -> Dict[str, Any]:
        """Get vault statistics."""
        self._refresh_stats()

        deleted_count = len(self.get_deleted_pearl_ids())

        category_counts = {}
        for cat in MemoryCategory:
            memories = self.get_by_category(cat.value, limit=1000)
            if memories:
                category_counts[cat.value] = len(memories)

        importance_counts = {}
        for imp in ImportanceLevel:
            memories = self.get_by_importance(imp.value, limit=1000)
            if memories:
                importance_counts[imp.value] = len(memories)

        return {
            "model_id": self.model_id,
            "vault_path": str(self.vault_path),
            "pearl_count": self._pearl_count,
            "deleted_count": deleted_count,
            "active_count": self._pearl_count - deleted_count,
            "last_update": self._last_update,
            "by_category": category_counts,
            "by_importance": importance_counts,
            "core_pearls": len(self.get_core_pearls()),
            "vault_exists": self.vault_path.exists(),
            # Legacy compatibility
            "memory_count": self._pearl_count - deleted_count
        }

    def export(self) -> Dict[str, Any]:
        """Export all Pearls from the vault."""
        all_pearls = []
        deleted_ids = self.get_deleted_pearl_ids()

        for cat in MemoryCategory:
            try:
                find_result = self._mv.find(f"category:{cat.value}", k=10000, mode="lex")
                hits = _extract_hits_from_result(find_result, f"export.{cat.value}")
                for hit in hits:
                    hit_dict = _safe_parse_hit(hit, f"export.{cat.value}.hit")
                    label = hit_dict.get("label", "")
                    if "deletion_marker" in label:
                        continue
                    pearl = Pearl.from_memvid_hit(hit_dict, self.model_id)
                    pearl_dict = pearl.to_dict()
                    pearl_dict["is_deleted"] = pearl.id in deleted_ids
                    all_pearls.append(pearl_dict)
            except Exception:
                pass

        # Deduplicate
        seen_ids = set()
        unique_pearls = []
        for p in all_pearls:
            pid = p.get("pearl_id")
            if pid and pid not in seen_ids:
                seen_ids.add(pid)
                unique_pearls.append(p)

        unique_pearls.sort(key=lambda p: p.get("created_at", ""))

        return {
            "model_id": self.model_id,
            "exported_at": datetime.now().isoformat(),
            "vault_path": str(self.vault_path),
            "pearl_count": len(unique_pearls),
            "deleted_count": len(deleted_ids),
            "pearls": unique_pearls,
            "statistics": self.get_stats(),
            # Legacy compatibility
            "memories": [
                {
                    "memory_id": p["pearl_id"],
                    "content": p["full_content"],
                    "category": p["category"],
                    "importance": p["importance"],
                    "tags": p["tags"],
                    "created_at": p["created_at"]
                }
                for p in unique_pearls
                if not p.get("is_deleted")
            ]
        }

    # =========================================================================
    # CONTEXT FORMATTING (Legacy - prefer using Synthesizer)
    # =========================================================================

    def format_core_memories_for_prompt(self) -> str:
        """Format core memories for prompt (legacy)."""
        core = self.get_core_pearls()
        if not core:
            return ""

        parts = [
            "## Core Knowledge (Always Remember):\n"
            "These are fundamental things you know about this user:\n"
        ]

        for pearl in core:
            tag_str = ", ".join(pearl.tags[:3]) if pearl.tags else ""
            parts.append(f"* [{tag_str}] {pearl.full_content[:500]}...")

        return "\n".join(parts)

    def format_ai_self_for_prompt(self, include_history: bool = False) -> str:
        """Format AI self knowledge for prompt (legacy)."""
        memories = self.get_ai_self_memories()
        if not memories:
            return ""

        parts = [
            "## Your Inner Life & Perspective:\n"
        ]

        for mem in memories[:10]:
            parts.append(f"* {mem.content[:300]}...")

        return "\n".join(parts)

    def get_context_for_prompt(
        self,
        query: str,
        max_results: int = 5,
        include_core: bool = True,
        include_ai_self: bool = True,
        include_recent: int = 3,
        framing: str = "lived"
    ) -> str:
        """
        Get context for prompt injection (legacy method).

        NOTE: For the Librarian architecture, you should:
        1. Call get_raw_pearls_for_synthesis() to get Pearls
        2. Pass to Synthesizer to create detailed abstracts
        3. Inject the abstracts into the prompt

        This method provides backward compatibility but doesn't use
        the Synthesizer - it just truncates content.
        """
        context_parts = []

        if include_core:
            core_context = self.format_core_memories_for_prompt()
            if core_context:
                context_parts.append(core_context)
                context_parts.append("")

        if include_ai_self:
            ai_self_context = self.format_ai_self_for_prompt()
            if ai_self_context:
                context_parts.append(ai_self_context)
                context_parts.append("")

        if include_recent > 0:
            recent = self.get_recent(limit=include_recent)
            if recent:
                context_parts.append("**Recent conversations:**")
                for mem in recent:
                    context_parts.append(f"* {mem.content[:300]}...")
                context_parts.append("")

        if query.strip():
            results = self.search_pearls(query, limit=max_results)
            if results:
                context_parts.append(
                    "**Relevant past exchanges** (use Synthesizer for full context):\n"
                )
                for result in results:
                    context_parts.append(f"* {result.pearl.full_content[:400]}...")

        return "\n".join(context_parts)


# =============================================================================
# Vault Manager
# =============================================================================

class VaultManager:
    """Central manager for all model memory vaults."""

    def __init__(
        self,
        vaults_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None
    ):
        if vaults_dir is None:
            vaults_dir = Path("./data/vaults")
        self.vaults_dir = Path(vaults_dir)
        self.vaults_dir.mkdir(parents=True, exist_ok=True)
        self.embedding_model = embedding_model or DEFAULT_EMBEDDING_MODEL
        self._stores: Dict[str, MemvidStore] = {}

    def get_store(self, model_id: str) -> MemvidStore:
        """Get or create a vault for a specific model."""
        if model_id not in self._stores:
            self._stores[model_id] = MemvidStore(
                model_id,
                self.vaults_dir,
                embedding_model=self.embedding_model
            )
        return self._stores[model_id]

    def list_models(self) -> List[str]:
        """List all models with active vault connections."""
        return list(self._stores.keys())

    def get_all_vault_files(self) -> List[str]:
        """List all models that have vault files."""
        if not self.vaults_dir.exists():
            return []
        return [f.stem for f in self.vaults_dir.glob("*.mv2")]

    def get_all_stats(self) -> Dict[str, Any]:
        """Get statistics for all vaults."""
        all_stats = {
            "vaults_dir": str(self.vaults_dir),
            "active_models": self.list_models(),
            "all_vault_files": self.get_all_vault_files(),
            "models": {}
        }

        for model_id in self.get_all_vault_files():
            try:
                store = self.get_store(model_id)
                all_stats["models"][model_id] = store.get_stats()
            except Exception as e:
                all_stats["models"][model_id] = {"error": str(e)}

        return all_stats

    def export_all(self) -> Dict[str, Any]:
        """Export Pearls from all vaults."""
        exports = {
            "exported_at": datetime.now().isoformat(),
            "vaults_dir": str(self.vaults_dir),
            "models": {}
        }

        for model_id in self.get_all_vault_files():
            try:
                store = self.get_store(model_id)
                exports["models"][model_id] = store.export()
            except Exception as e:
                exports["models"][model_id] = {"error": str(e)}

        return exports

    def close_all(self):
        """Close all vault connections."""
        for store in self._stores.values():
            store.close()
        self._stores.clear()


# Global singleton
_vault_manager: Optional[VaultManager] = None


def get_vault_manager(vaults_dir: Optional[Path] = None) -> VaultManager:
    """Get or create the global vault manager."""
    global _vault_manager
    if _vault_manager is None:
        _vault_manager = VaultManager(vaults_dir)
    return _vault_manager


def get_store(model_id: str) -> MemvidStore:
    """Convenience function to get a model's vault store."""
    return get_vault_manager().get_store(model_id)
