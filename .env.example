# =============================================================================
# GAM-Memvid Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your API keys:
#   cp .env.example .env
#
# Required: OPENAI_API_KEY (for Synthesizer)
# =============================================================================

# =============================================================================
# OpenAI API (REQUIRED for Synthesizer)
# =============================================================================
# The Synthesizer uses gpt-4o-mini to create detailed abstracts at retrieval time.
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here

# Optional: Custom OpenAI-compatible endpoint (e.g., Azure, local proxy)
# OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# Condenser Configuration (Optional - for tag extraction)
# =============================================================================
# The MemoryCondenser can use gpt-4o-mini for tag extraction.
# Uses same OPENAI_API_KEY as above.
CONDENSER_PROVIDER=openai
CONDENSER_MODEL=gpt-4o-mini

# Optional: Use Gemini instead (cost effective alternative)
# GEMINI_API_KEY=AIza-your-key-here
# CONDENSER_PROVIDER=gemini
# CONDENSER_MODEL=gemini-2.0-flash

# =============================================================================
# Embedding Model (Local - No API Key Needed)
# =============================================================================
# fastembed runs locally, no API key required
# Default: BAAI/bge-small-en-v1.5 (fast, ~50MB download on first run)
# Options: BAAI/bge-base-en-v1.5, sentence-transformers/all-MiniLM-L6-v2
MEMVID_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# =============================================================================
# Server Configuration
# =============================================================================
# Railway will override PORT via environment variable
HOST=0.0.0.0
PORT=8100

# =============================================================================
# Data Storage
# =============================================================================
# Where to store vault files and data.
# Local: ./data
# Railway: set DATA_DIR to the volume mount path (e.g. /app/data) and mount a volume there.
# DATA_DIR=./data
# DATA_DIR=/app/data
DATA_DIR=./data
VAULTS_DIR=

# =============================================================================
# Continuum Bridge (OpenWebUI for journal/scheduler)
# =============================================================================
# When Continuum app uses the bridge, the server calls OpenWebUI to resolve
# thread_id -> model_id and to post prompts. Set these to your OpenWebUI instance.
# CONTINUUM_OPENWEBUI_BASE_URL=https://your-openwebui.railway.app
# CONTINUUM_OPENWEBUI_API_KEY=sk-your-openwebui-api-key
# Optional: require Authorization: Bearer <key> or X-API-Key on /continuum/* requests.
# Generate with: openssl rand -hex 32
# Set the same value in Continuum Settings > Bridge API Key.
# CONTINUUM_BRIDGE_API_KEY=your-secret-bridge-key

# DATA_DIR is used for vaults and for Continuum persistence (DATA_DIR/continuum/).
# On Railway, mount a volume at DATA_DIR so schedules and settings persist.

# =============================================================================
# User name (synthesizer / memories)
# =============================================================================
# Used when the client doesn't send user_name (e.g. continuum journal trigger).
# Set to match the name in OpenWebUI valve USER_NAME so the AI uses your name.
# DEFAULT_USER_NAME=User
